{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gapfilling NDVI/LST with machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from joblib import dump\n",
    "from scipy import stats\n",
    "import geopandas as gpd\n",
    "from pprint import pprint\n",
    "from odc.geo.xr import assign_crs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import shap\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools/')\n",
    "from dea_tools.classification import predict_xr, HiddenPrints\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _collect_prediction_data import round_coords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "* `path`: The path to the input shapefile. A default shapefile is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var='LST'#'NDVI'\n",
    "n_samples = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble datasets for training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/climate-carbon-interactions/data/5km/'\n",
    "\n",
    "datasets = [\n",
    "    model_var+'_5km_monthly_1982_2022_wGaps.nc',\n",
    "     # 'NDVI_5km_monthly_1982_2022.nc',\n",
    "    'rain_5km_monthly_1981_2022.nc',\n",
    "    'rain_cml3_5km_monthly_1982_2022.nc',\n",
    "    'rain_cml6_5km_monthly_1982_2022.nc',\n",
    "    'rain_cml12_5km_monthly_1982_2022.nc',\n",
    "    'srad_5km_monthly_1982_2022.nc',\n",
    "    'tavg_5km_monthly_1982_2022.nc',\n",
    "    'vpd_5km_monthly_1982_2022.nc',\n",
    "    'MOY_5km_monthly_1982_2022.nc',\n",
    "    'Elevation_5km_monthly_1982_2022.nc',\n",
    "    'CO2_5km_monthly_1982_2022.nc',\n",
    "    'WCF_5km_monthly_1990_2022.nc'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = []\n",
    "for d in datasets:\n",
    "    xx = xr.open_dataset(base+d).sel(time=slice('1990','2021'))\n",
    "    xx = assign_crs(xx, crs ='epsg:4326')\n",
    "    xx = round_coords(xx)\n",
    "    xx = xx.drop('spatial_ref')\n",
    "    dss.append(xx)\n",
    "\n",
    "ds = xr.merge(dss)\n",
    "ds = assign_crs(ds, crs ='epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing data: equal random sampling of bioclimatic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/NEE_modelling/data/bioclimatic_regions.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save results \n",
    "results = []\n",
    "for index, row in gdf.iterrows():\n",
    "    print(row['region_name'])\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds[model_var])\n",
    "    mask = round_coords(mask)\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    dss = ds.where(mask)\n",
    "\n",
    "    #sample equivalent num of samples per region\n",
    "    df = dss.to_dataframe().dropna().sample(n=int(n_samples/len(gdf)), random_state=0).reset_index()\n",
    "    \n",
    "    # Append results to a dictionary using the attribute\n",
    "    # column as an key\n",
    "    results.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(results).reset_index(drop=True)\n",
    "df['year'] = pd.DatetimeIndex(df['time']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['time', 'spatial_ref'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = df.sample(n=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(validation.index)\n",
    "print(len(df), 'training samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the location of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf_val = gpd.GeoDataFrame(\n",
    "    validation, geometry=gpd.points_from_xy(validation.longitude, validation.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# gdf_val.explore(column='year', cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file('/g/data/os22/chad_tmp/climate-carbon-interactions/data/training_data.geojson')\n",
    "gdf_val.to_file('/g/data/os22/chad_tmp/climate-carbon-interactions/data/validation_data.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training data if skipping above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/climate-carbon-interactions/data/training_data.geojson')\n",
    "df = pd.DataFrame(gdf.drop(columns='geometry', axis=1))\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "gdf_val = gpd.read_file('/g/data/os22/chad_tmp/climate-carbon-interactions/data/validation_data.geojson')\n",
    "validation = pd.DataFrame(gdf_val.drop(columns='geometry', axis=1))\n",
    "validation = validation.drop(['year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[model_var]\n",
    "x = df.drop([model_var, 'longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model using nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid using distributions\n",
    "param_grid = {\n",
    "    'num_leaves': stats.randint(5,50),\n",
    "    'min_child_samples':stats.randint(10,30),\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'max_depth': stats.randint(5,25),\n",
    "    'n_estimators': [200, 300, 400, 500],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True,\n",
    "                   random_state=0)\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "rmse=[]\n",
    "r2=[]\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in outer_cv.split(x, y):\n",
    "    print(f\"Working on {i}/5 outer CV split\", end='\\r')\n",
    "    model = LGBMRegressor(random_state=1,\n",
    "                          verbose=-1,\n",
    "                          # n_jobs=-1\n",
    "                          )\n",
    "\n",
    "    # index training, testing\n",
    "    X_tr, X_tt = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "    y_tr, y_tt = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    #simple random split on inner fold\n",
    "    inner_cv = KFold(n_splits=3,\n",
    "                     shuffle=True,\n",
    "                     random_state=0)\n",
    "    \n",
    "    clf = RandomizedSearchCV(\n",
    "                   model,\n",
    "                   param_grid,\n",
    "                   verbose=0,\n",
    "                   n_iter=100,\n",
    "                   # n_jobs=-1,\n",
    "                   cv=inner_cv.split(X_tr, y_tr)\n",
    "                  )\n",
    "    \n",
    "    #prevents extensive print statements\n",
    "    clf.fit(X_tr, y_tr, callbacks=None)\n",
    "    \n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # r2\n",
    "    r2_ = r2_score(y_tt, pred)\n",
    "    r2.append(r2_)\n",
    "    # MAE\n",
    "    ac = mean_absolute_error(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # RMSE\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_tt, pred))\n",
    "    rmse.append(rmse_)\n",
    "    \n",
    "    #1:1 plots for each fold (save to csv so we can make a plot later on)\n",
    "    df = pd.DataFrame({'Test':y_tt, 'Pred':pred}).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(\"/g/data/os22/chad_tmp/climate-carbon-interactions/results/cross_validation/\"+str(i)+\"_\"+model_var+\"_lgbm.csv\")\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single 1:1 plot out of the folds \n",
    "\n",
    "None of the test samples overlap between folds, and every sample has been tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffs=[]\n",
    "for i in range(1,5+1):\n",
    "    df = pd.read_csv('/g/data/os22/chad_tmp/climate-carbon-interactions/results/cross_validation/'i+'_'+model_var+'_lgbm.csv', usecols=['Test', 'Pred'])\n",
    "    dffs.append(df)\n",
    "\n",
    "cross_df = pd.concat(dffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "xy = np.vstack([cross_df['Test'],cross_df['Pred']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=cross_df, x='Test',y='Pred',c=z, s=50, lw=1, alpha=0.5, ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Pred', scatter=False, color='darkblue', ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Test', color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "\n",
    "plt.xlabel('Observation '+model_var, fontsize=16)\n",
    "plt.ylabel('Prediction '+model_var, fontsize=16)\n",
    "ax.text(.05, .95, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(np.mean(r2)),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.text(.05, .9, 'MAE={:.2g}'.format(np.mean(acc)),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/cross_val_\"+model_var+\"_lgbm_\"+suffix+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using all training data\n",
    "\n",
    "Using a randomized strategy so we can search through more variables, with 500 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True,\n",
    "                   random_state=0)\n",
    "\n",
    "clf = RandomizedSearchCV(LGBMRegressor(verbose=-1),\n",
    "                   param_grid,\n",
    "                   verbose=1,\n",
    "                   n_iter=500,\n",
    "                   # n_jobs=-1,\n",
    "                   cv=outer_cv\n",
    "                  )\n",
    "\n",
    "clf.fit(x, y, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The best score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all data using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(**clf.best_params_)\n",
    "\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with independent validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = validation[model_var]\n",
    "x_val = validation.drop([model_var,'longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)\n",
    "\n",
    "r2 = r2_score(y_val, pred)\n",
    "ac = mean_absolute_error(y_val, pred)\n",
    "df_val = pd.DataFrame({'Test':y_val, 'Pred':pred}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "xy = np.vstack([df_val['Test'],df_val['Pred']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=df_val, x='Test',y='Pred',c=z, s=50, lw=1, alpha=0.5, ax=ax)\n",
    "sb.regplot(data=df_val, x='Test',y='Pred', scatter=False, color='darkblue', ax=ax)\n",
    "sb.regplot(data=df_val, x='Test',y='Test', color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "\n",
    "plt.xlabel('Observation '+model_var, fontsize=16)\n",
    "plt.ylabel('Prediction '+model_var, fontsize=16)\n",
    "ax.text(.05, .95, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(r2),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.text(.05, .9, 'MAE={:.2g}'.format(ac),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/cross_val_\"+model_var+\"_lgbm_\"+suffix+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/g/data/os22/chad_tmp/climate-carbon-interactions/results/models/gapfill/gapfill_'+model_var+'_LGBM.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance using SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals= np.abs(shap_values.values).mean(0)\n",
    "feature_importance = pd.DataFrame(list(zip(x.columns, vals)), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "feature_importance['col_name'] = feature_importance['col_name'].str.removesuffix(\"_RS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,7))\n",
    "shap.summary_plot(shap_values, max_display=11, show=False, feature_names=feature_importance['col_name'])\n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "plt.gcf().axes[-1].set_box_aspect(15) \n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "ax.set_xlabel(model_var+' SHAP Value', fontsize=16)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/feature_importance_\"+model_var+\"_lgbm_\"+suffix+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from datacube.utils.dask import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var='LST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('/g/data/os22/chad_tmp/climate-carbon-interactions/results/models/gapfill/gapfill_'+model_var+'_LGBM.joblib').set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prediction data\n",
    "\n",
    "and index to match training data order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/climate-carbon-interactions/data/5km/'\n",
    "\n",
    "datasets = [\n",
    "    model_var+'_5km_monthly_1982_2022_wGaps.nc',\n",
    "     # 'NDVI_5km_monthly_1982_2022.nc',\n",
    "    'rain_5km_monthly_1981_2022.nc',\n",
    "    'rain_cml3_5km_monthly_1982_2022.nc',\n",
    "    'rain_cml6_5km_monthly_1982_2022.nc',\n",
    "    'rain_cml12_5km_monthly_1982_2022.nc',\n",
    "    'srad_5km_monthly_1982_2022.nc',\n",
    "    'tavg_5km_monthly_1982_2022.nc',\n",
    "    'vpd_5km_monthly_1982_2022.nc',\n",
    "    'MOY_5km_monthly_1982_2022.nc',\n",
    "    'Elevation_5km_monthly_1982_2022.nc',\n",
    "    'CO2_5km_monthly_1982_2022.nc',\n",
    "    'WCF_5km_monthly_1990_2022.nc'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = []\n",
    "for d in datasets:\n",
    "    xx = xr.open_dataset(base+d).sel(time=slice('1990','2021'))\n",
    "    xx = assign_crs(xx, crs ='epsg:4326')\n",
    "    xx = round_coords(xx)\n",
    "    xx = xx.drop('spatial_ref')\n",
    "    dss.append(xx)\n",
    "\n",
    "ds = xr.merge(dss)\n",
    "ds = assign_crs(ds, crs ='epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add latitude as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = ds.latitude\n",
    "lat = lat.expand_dims(time=ds.time, longitude=ds.longitude)\n",
    "lat = lat.transpose('time', 'latitude', 'longitude')\n",
    "ds['latitude_gridded'] = lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(ds.data_vars)[1:-1]\n",
    "columns.insert(0, 'latitude_gridded')\n",
    "ds = ds[columns]\n",
    "ds = ds.rename({'latitude':'y', 'longitude':'x'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(ds.WCF.sel(time='2015').mean('time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i=0\n",
    "for i in range(0, len(ds.time)):\n",
    "    print(\" {:03}/{:03}\\r\".format(i + 1, len(range(0, len(ds.time)))), end=\"\")\n",
    "    with HiddenPrints():\n",
    "        predicted = predict_xr(model,\n",
    "                            ds.isel(time=i),\n",
    "                            proba=False,\n",
    "                            clean=True,\n",
    "                            chunk_size=10000,\n",
    "                              ).compute()\n",
    "    \n",
    "    # predicted = predicted.Predictions.where(~mask.isel(time=i))\n",
    "    predicted['time'] = ds.isel(time=i).time.values\n",
    "    results.append(predicted.astype('float32'))\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = xr.concat(results, dim='time').sortby('time').rename({'Predictions':model_var})#.astype('float32')\n",
    "yy['time'] = ds.isel(time=range(0, len(yy.time))).time\n",
    "yy = yy.where(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.to_netcdf('/g/data/os22/chad_tmp/climate-carbon-interactions/results/ml_predictions/NDVI_predicted_5km_monthly_1990_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = xr.open_dataset('/g/data/os22/chad_tmp/climate-carbon-interactions/data/5km/'+model_var+'_5km_monthly_1982_2022_wGaps.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = ndvi.sel(time=yy.time)\n",
    "ndvi = ndvi.NDVI.rename({'latitude':'y', 'longitude':'x'})\n",
    "gaps_mask = ~np.isnan(ndvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = yy.NDVI - ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(13,5))\n",
    "yy.NDVI.where(gaps_mask).mean(['x','y']).plot(ax=ax, label='predictions')\n",
    "ndvi.mean(['x','y']).plot(ax=ax, label='observed')\n",
    "ax.legend()\n",
    "ax.set_title('Aus-Wide NDVI matching data gaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = xr.corr(ndvi, yy.NDVI, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = yy.NDVI.mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean = ndvi.mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(10,10), sharey=True)\n",
    "ndvi_mean.plot.imshow(ax=ax[0,0], vmin=0.05, vmax=0.7, add_labels=False)\n",
    "pred_mean.plot.imshow(ax=ax[0,1], vmin=0.05, vmax=0.7, add_labels=False)\n",
    "(ndvi_mean - pred_mean).plot.imshow(ax=ax[1,0], cmap='RdBu', vmin=-0.1, vmax=0.1, add_labels=False)\n",
    "corr.plot.imshow(ax=ax[1,1], cmap='magma', robust=True, add_labels=False)\n",
    "ax[0,0].set_title('Observed Mean NDVI (1990-2021)')\n",
    "ax[0,1].set_title('Predicted Mean NDVI (1990-2021)')\n",
    "ax[1,0].set_title('Difference (Obs-Pred)')\n",
    "ax[1,1].set_title('Correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gapfill with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1619aa7c0a0d41f892bc55d700874aba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ef7c7b47129f4bb7be210a31c9c6748c",
       "style": "IPY_MODEL_6c45957fae8a4c9c90d0efa71fb7acd3",
       "value": "100%"
      }
     },
     "1b9dd35d726a487c9e03b50cadec204e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27c2edfba16545549cbc2446f65308f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "43e325ac2f4b42659d9ae321464c5dd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9b47cb4d6d96465b8a2446bc022c085d",
       "max": 217,
       "style": "IPY_MODEL_27c2edfba16545549cbc2446f65308f9",
       "value": 217
      }
     },
     "5ce2388f84c64f1eb830509fab75e200": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1619aa7c0a0d41f892bc55d700874aba",
        "IPY_MODEL_43e325ac2f4b42659d9ae321464c5dd1",
        "IPY_MODEL_e6391b33874b4787837251849ceb2b1c"
       ],
       "layout": "IPY_MODEL_cc67c362ea5f4f80bfa29df7189ad3e6"
      }
     },
     "6c45957fae8a4c9c90d0efa71fb7acd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "91df15d5486440d6a2125e74e9befe48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9b47cb4d6d96465b8a2446bc022c085d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cc67c362ea5f4f80bfa29df7189ad3e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6391b33874b4787837251849ceb2b1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1b9dd35d726a487c9e03b50cadec204e",
       "style": "IPY_MODEL_91df15d5486440d6a2125e74e9befe48",
       "value": " 217/217 [01:53&lt;00:00,  2.18it/s]"
      }
     },
     "ef7c7b47129f4bb7be210a31c9c6748c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
